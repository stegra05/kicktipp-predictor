### 2\. Enhance Recency in Feature Engineering

  * **Concept üß†:** While weighted data tells the model *which matches* are important, modifying features tells the model *which patterns* are important. The goal is to give the model both a short-term and long-term "memory." The volatility in your matchday performance suggests the model is missing sharp, recent changes in team form.

  * **How to Implement üë®‚Äçüíª:** This involves making changes in `src/kicktipp_predictor/data.py` to create new features that are more sensitive to recent performance.

    1.  **Add Short-Term Form Windows:** In your `_get_form_features` function, you currently use `last_n: int = 5`. You can create additional features based on a shorter window.

        ```python
        # In data.py, inside _create_match_features

        # Existing long-term form
        features.update(self._get_form_features(home_team, home_history, prefix='home_long_term', last_n=10))

        # Standard form
        features.update(self._get_form_features(home_team, home_history, prefix='home', last_n=5))

        # New short-term "hot streak" form
        features.update(self._get_form_features(home_team, home_history, prefix='home_short_term', last_n=2))
        ```

    2.  **Use Exponentially Weighted Moving Averages (EWMA):** This is a more elegant method than using multiple windows. It creates a smoothed average where recent data points have exponentially more weight. You can use this for stats like goals scored/conceded.

        ```python
        # This is more conceptual, but in your feature engineering...
        # Instead of a simple rolling mean for a team's goals:
        team_stats['avg_goals'] = team_stats['goals_scored'].rolling(window=5).mean()

        # You would use an EWMA:
        team_stats['ewma_goals'] = team_stats['goals_scored'].ewm(span=5, adjust=False).mean()
        ```

        The `span` parameter is roughly equivalent to the window size. This creates a feature that is much more responsive to the latest match result.

  * **Complexity:** **Low to Medium**. It involves modifying your feature generation code in `data.py`. While not difficult, it requires careful handling of your pandas DataFrames to ensure calculations are correct.

  * **Estimated Effect:** **Medium**. Providing the model with more nuanced, time-sensitive features allows it to better distinguish between a team's baseline quality (long-term form) and its immediate momentum (short-term form). This should help reduce the volatility seen in the per-matchday summary.

-----

### 3\. Bayesian Hierarchical Models

  * **Concept üß†:** This is a fundamental shift in your modeling approach. Instead of having one "true" skill value for a team, a Bayesian model treats a team's attack and defense strength as **probability distributions**. The "hierarchical" part means that each team's skill is assumed to be drawn from a common, league-wide distribution.

      * **Analogy:** Imagine estimating the skill of a new player. You start with a general belief about the *average skill of all players in the league* (the hierarchy). When they play their first game, you don't completely trust that single result. Instead, you slightly update your belief about them, pulling it away from the league average. As they play more games, your belief about their specific skill becomes stronger and relies less on the league average.

    This is perfect for the "cold start" problem at the beginning of a season, where you have little data. It provides a principled way to combine prior knowledge with new evidence.

  * **How to Implement üë®‚Äçüíª:** This requires moving away from XGBoost and using a probabilistic programming library like **PyMC** (Python) or **Stan**.

    1.  **Define the Model Structure:**
          * Create parameters for each team's attack strength (`atts_team_i`) and defense strength (`defs_team_i`).
          * Define these parameters as being drawn from a common normal distribution (e.g., `Normal(mu_atts, sigma_atts)`). This is the hierarchy.
          * Define priors for the league-wide mean and standard deviation (e.g., `mu_atts`, `sigma_atts`).
    2.  **Define the Likelihood:** The number of goals a team scores in a match is modeled with a **Poisson distribution**. The rate (`lambda`) of the Poisson is determined by the home team's attack, the away team's defense, and a home-field advantage term.
    3.  **Run Inference:** Use a Markov Chain Monte Carlo (MCMC) sampler to find the posterior distributions for every team's attack and defense strength based on the match data.
    4.  **Make Predictions:** To predict a future match, you sample from the posterior distributions of the two teams' strengths to simulate the match thousands of times, allowing you to calculate the probability of a home win, draw, or away win.

  * **Complexity:** **High**. This is a significant undertaking. It requires learning a new modeling paradigm, a new library, and the concepts of Bayesian inference (priors, posteriors, MCMC). It's a fantastic learning project but is much more involved than the other two methods.

  * **Estimated Effect:** **High**. If implemented well, this approach is often considered state-of-the-art for sports modeling. It is robust to sparse data, provides full probability distributions for outcomes, and gives you a measure of uncertainty for every parameter.